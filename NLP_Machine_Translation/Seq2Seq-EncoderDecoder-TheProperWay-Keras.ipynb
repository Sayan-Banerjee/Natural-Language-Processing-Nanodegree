{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import helper\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_texts = []\n",
    "target_texts_inputs = []\n",
    "for s in french_sentences:\n",
    "    target_text = s + ' <eos>'\n",
    "    target_text_input = '<sos> ' + s\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the inputs\n",
    "tokenizer_inputs = Tokenizer()\n",
    "tokenizer_inputs.fit_on_texts(english_sentences)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "\n",
    "# store number of input words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_input = len(word2idx_inputs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the outputs\n",
    "# don't filter out special characters\n",
    "# otherwise <sos> and <eos> won't appear\n",
    "tokenizer_outputs = Tokenizer(filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 356 unique output tokens.\n"
     ]
    }
   ],
   "source": [
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(word2idx_outputs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (137861, 15)\n",
      "encoder_inputs[0]: [ 0  0 17 23  1  8 67  4 39  7  3  1 55  2 44]\n",
      "decoder_inputs[0]: [  7  38  37   1  12  70  40  15  28   3  10   5   1 115   4  53   2   0\n",
      "   0   0   0   0   0   0]\n",
      "decoder_inputs.shape: (137861, 24)\n",
      "decoder_targets[0]: [ 38  37   1  12  70  40  15  28   3  10   5   1 115   4  53   2   6   0\n",
      "   0   0   0   0   0   0]\n",
      "decoder_targets.shape: (137861, 24)\n",
      "After reshaping, decoder_targets.shape: (137861, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input, padding='pre')\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_targets[0]:\", decoder_targets[0])\n",
    "print(\"decoder_targets.shape:\", decoder_targets.shape)\n",
    "\n",
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "decoder_targets = decoder_targets.reshape(*decoder_targets.shape, 1)\n",
    "print(\"After reshaping, decoder_targets.shape:\", decoder_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encdec_model(input_sequence_length, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    dropout = 0.5\n",
    "    embeddim = 200\n",
    "    outputdim = french_vocab_size\n",
    "    rnnunits = 128\n",
    "    input_sequence = Input(shape=(input_sequence_length,))\n",
    "    embedding_layer = Embedding(\n",
    "                      english_vocab_size,\n",
    "                      embeddim,\n",
    "                      embeddings_initializer=\"glorot_normal\",\n",
    "                      input_length=input_sequence_length,\n",
    "                      trainable=True\n",
    "                        )\n",
    "    x = embedding_layer(input_sequence)\n",
    "    encoder = GRU(units=rnnunits, return_state=True)\n",
    "    \n",
    "    encoder_outputs, state_h = encoder(x)\n",
    "    encoder_states = [state_h]\n",
    "    decoder_inputs = Input(shape=(output_sequence_length,))\n",
    "    decoder_embedding = Embedding(french_vocab_size, embeddim,\n",
    "                                  embeddings_initializer=\"glorot_normal\",\n",
    "                                  trainable = True)\n",
    "    decoder_inputs_x = decoder_embedding(decoder_inputs)\n",
    "    decoder_gru = GRU(rnnunits, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _ = decoder_gru(decoder_inputs_x, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(units=outputdim, activation='softmax')\n",
    "    logits = decoder_dense(decoder_outputs)\n",
    "    model = Model([input_sequence, decoder_inputs], logits)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=1e-3),\n",
    "                  metrics=['accuracy'])\n",
    "    return model, input_sequence, encoder_states, decoder_embedding, decoder_gru, decoder_dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, input_sequence, encoder_states, decoder_embedding, decoder_gru, decoder_dense =\\\n",
    "                                                                encdec_model(input_sequence_length = max_len_input,\n",
    "                                                                output_sequence_length= max_len_target,\n",
    "                                                                english_vocab_size = num_words_input,\n",
    "                                                                french_vocab_size = num_words_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/25\n",
      "110288/110288 [==============================] - 52s 471us/step - loss: 1.0255 - acc: 0.7331 - val_loss: 0.5059 - val_acc: 0.8280\n",
      "Epoch 2/25\n",
      "110288/110288 [==============================] - 51s 464us/step - loss: 0.4305 - acc: 0.8531 - val_loss: 0.3752 - val_acc: 0.8704\n",
      "Epoch 3/25\n",
      "110288/110288 [==============================] - 51s 464us/step - loss: 0.3367 - acc: 0.8833 - val_loss: 0.2969 - val_acc: 0.8981\n",
      "Epoch 4/25\n",
      "110288/110288 [==============================] - 51s 464us/step - loss: 0.2584 - acc: 0.9106 - val_loss: 0.2384 - val_acc: 0.9149\n",
      "Epoch 5/25\n",
      "110288/110288 [==============================] - 51s 465us/step - loss: 0.2024 - acc: 0.9292 - val_loss: 0.1864 - val_acc: 0.9349\n",
      "Epoch 6/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.1770 - acc: 0.9370 - val_loss: 0.1661 - val_acc: 0.9417\n",
      "Epoch 7/25\n",
      "110288/110288 [==============================] - 51s 464us/step - loss: 0.1474 - acc: 0.9495 - val_loss: 0.1240 - val_acc: 0.9594\n",
      "Epoch 8/25\n",
      "110288/110288 [==============================] - 51s 462us/step - loss: 0.0914 - acc: 0.9700 - val_loss: 0.0686 - val_acc: 0.9786\n",
      "Epoch 9/25\n",
      "110288/110288 [==============================] - 51s 464us/step - loss: 0.0568 - acc: 0.9812 - val_loss: 0.0493 - val_acc: 0.9846\n",
      "Epoch 10/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0434 - acc: 0.9859 - val_loss: 0.0373 - val_acc: 0.9896\n",
      "Epoch 11/25\n",
      "110288/110288 [==============================] - 51s 464us/step - loss: 0.0327 - acc: 0.9904 - val_loss: 0.0292 - val_acc: 0.9923\n",
      "Epoch 12/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0252 - acc: 0.9929 - val_loss: 0.0255 - val_acc: 0.9929\n",
      "Epoch 13/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0226 - acc: 0.9932 - val_loss: 0.0222 - val_acc: 0.9936\n",
      "Epoch 14/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0298 - val_acc: 0.9900\n",
      "Epoch 15/25\n",
      "110288/110288 [==============================] - 51s 462us/step - loss: 0.0173 - acc: 0.9947 - val_loss: 0.0255 - val_acc: 0.9915\n",
      "Epoch 16/25\n",
      "110288/110288 [==============================] - 51s 462us/step - loss: 0.0163 - acc: 0.9950 - val_loss: 0.0168 - val_acc: 0.9950\n",
      "Epoch 17/25\n",
      "110288/110288 [==============================] - 51s 462us/step - loss: 0.0159 - acc: 0.9951 - val_loss: 0.0167 - val_acc: 0.9952\n",
      "Epoch 18/25\n",
      "110288/110288 [==============================] - 51s 462us/step - loss: 0.0133 - acc: 0.9958 - val_loss: 0.0168 - val_acc: 0.9951\n",
      "Epoch 19/25\n",
      "110288/110288 [==============================] - 51s 462us/step - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0152 - val_acc: 0.9956\n",
      "Epoch 20/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0130 - acc: 0.9958 - val_loss: 0.0143 - val_acc: 0.9958\n",
      "Epoch 21/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0185 - val_acc: 0.9945\n",
      "Epoch 22/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0136 - val_acc: 0.9960\n",
      "Epoch 23/25\n",
      "110288/110288 [==============================] - 51s 464us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0138 - val_acc: 0.9960\n",
      "Epoch 24/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0129 - val_acc: 0.9962\n",
      "Epoch 25/25\n",
      "110288/110288 [==============================] - 51s 463us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0124 - val_acc: 0.9963\n"
     ]
    }
   ],
   "source": [
    "r = model.fit([encoder_inputs, decoder_inputs], decoder_targets, batch_size=128, epochs=25, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2344: UserWarning: Layer gru_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'gru_1/while/Exit_2:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make predictions #####\n",
    "# We need to create another model\n",
    "# that can take in the RNN state and previous word as input\n",
    "# and accept a T=1 sequence.\n",
    "\n",
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "rnnunits = 128\n",
    "encoder_model = Model(input_sequence, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(rnnunits,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# this time, we want to keep the states too, to be output\n",
    "# by our sampling model\n",
    "\n",
    "decoder_outputs, state_h = decoder_gru(\n",
    "  decoder_inputs_single_x,\n",
    "  initial_state=decoder_states_inputs\n",
    ") \n",
    "decoder_states = [state_h]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# The sampling model\n",
    "# inputs: y(t-1), h(t-1), c(t-1)\n",
    "# outputs: y(t), h(t), c(t)\n",
    "decoder_model = Model(\n",
    "  [decoder_inputs_single] + decoder_states_inputs, \n",
    "  [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    states_value = [states_value]\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # NOTE: tokenizer lower-cases all words\n",
    "    idx = word2idx_outputs['<sos>']\n",
    "    \n",
    "    # if we get this we break\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "\n",
    "    # Create the translation\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = idx\n",
    "        \n",
    "        output_tokens, h = decoder_model.predict([target_seq] + states_value)\n",
    "        # Get next word\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h]\n",
    "        \n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: france is usually busy during october , and it is usually pleasant in summer .\n",
      "Translation: la france est généralement occupé en octobre , et il est généralement agréable en été .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: china is never pleasant during march , but it is usually dry in november .\n",
      "Translation: chine est jamais agréable au mois de mars , mais il est généralement sec en novembre .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: mice are your favorite animals .\n",
      "Translation: souris sont vos animaux préférés .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: strawberries are your most loved fruit .\n",
      "Translation: les fraises sont vos fruits les plus aimés .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: california is snowy during november , but it is usually busy in winter .\n",
      "Translation: californie est neigeux au mois de novembre , mais il est généralement occupé en hiver .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: i dislike bananas , apples , and pears .\n",
      "Translation: je n'aime les bananes , les pommes et les poires .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: her most loved fruit is the banana , but his most loved is the mango .\n",
      "Translation: son fruit le plus aimé est la banane , mais son plus aimé est la mangue .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: the united states is sometimes rainy during may , and it is usually busy in autumn .\n",
      "Translation: les états-unis est parfois pluvieux au mois de mai , et il est généralement occupé à l' automne .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: the united states is freezing during winter , and it is never hot in spring .\n",
      "Translation: les états-unis est le gel pendant l' hiver , et il est jamais chaud au printemps .\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input: our least liked fruit is the peach , but their least liked is the orange .\n",
      "Translation: notre moins aimé des fruits est la pêche , mais leur moins aimé est l'orange .\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  # Do some test translations\n",
    "    i = np.random.choice(len(english_sentences))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', english_sentences[i])\n",
    "    print('Translation:', translation)\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
